# 机器学习基础

* 代码在opencv_contrib中
* 深度神经网络库cnn_3dobj和dnn
* OpenCV主要处理统计机器学习
* 机器学习中常见的任务**分类和聚类**与计算机视觉中最常见的两个任务**识别和分割**有共通之处。
* OpenCV的ML库: /opencv/modules/ml 和 /opencv/modules/flann

## 生成式模型和判别式模型
OpenCV倾向于支持判别式算法。该类算法是通过给定数据来判断类别的概率P(L|D)。而生成式算法
	是通过给定标签来生成数据的分布P(D|L)。判别式模型在根据给定数据做出预测方面更有优势，
	而生成式模型则具有更好的数据表达能力，在有条件的生成新数据方面也更有优势。

生成式模型通常根据给定的数据类别得到数据的条件分布，因此可以很容易看出这个分布跟实际的分布
	是否相似。生成式模型是对数据的产生进行建模，通常容易理解。而判别式模型则经常根据一些阙
	值作出决定。

## OpenCV的机器学习算法
* Mahalanobis算法， K-均值算法在core库中
* 目标识别和人脸识别在objdetect库中

* Mahalanobis, 一个距离度量。通过除以协方差来对数据空间进行变换，得到一个标志着空间拉伸程度
	的距离度量。如果协方差矩阵是单位矩阵则等价于欧式距离

* K-均值算法，非监督的聚类算法，用k个聚类中心表示数据的分布。该方法与期望最大化算法的区别在
	于k均值的中心不符合高斯分布。每个聚类中心都竞争者去俘获与之距离较近的点，所以聚类的结果
	就像肥皂泡，聚类区域经常被用作稀疏直方图的bin，用来描述数据。

* 正太/朴素贝叶斯，生成式的分类器。假设特征符合高斯分布并在统计学上服从独立同分布，但这个条
	件过于苛刻以至于在很多情况下不能被满足。因此被称为"朴素"贝叶斯分类器。尽管如此，它的效
	果仍然不错。

* 决策树，判别分类器。该树在当前节点通过寻找数据特征和一个阙值来实现数据到不同类别的最优划分。
	处理流程是不断将数据进行划分，然后下降到树的左侧子节点或右侧子节点。该算法性能不是最好的，
	但往往是测试的首选，它运行速度快，性能好。

* 期望最大化算法EM，一种用于聚类的无监督算法。它用N维高斯分布去拟合数据。仅用很少的几个参数
	(方差和均值) 就可以表示一个较为复杂的数据分布。该方法常用于分割。

* Boosting, 由一组判别分类器组成。最终的决策由组成它的各个分类器的加权组合来决定。在训练中，
	逐个训练子分类器。且每个子分类器都是弱分类器(性能上只是优于随机选择)。这些弱分类器由单
	变量决策树构成，被称为"桩"(stumps)。 在训练中，决策桩(decision stump)不仅从数据中学习
	分类决策而且根据预测精度学习"投票"的权重。当训练下一个分类器的时候，数据样本的权重会被重
	新分配。使之能够给与分错的数据更多的注意力。训练过程不停地执行，直到总错误(加权组合所有
	决策树组成的分类器产生的错误)低于预设的阙值。为了达到好的效果，这个方法通常需要很大量训
	练的训练数据。

* 随机森林，由很多决策树组成，每个决策树向下递归以获取最大深度。在学习过程中，每棵树的每个节
	点只从特征数据的一个随机子集中选择。这保证了每棵树是统计意义上不相关的分类器。在识别过程
	中，将每棵树的结果进行投票来确定最终结果，每棵树的权重相同。这个分类器方法经常很有效，而
	且对每棵树的输出进行平均，可以处理回归问题。

* k近邻，训练数据跟类别标签放在一起，离测试数据最近(欧氏距离)的k个样本进行投票，确定测试数据
	的分类结果。该方法通常比较有效，但是速度较慢且内存需求较大。

* 快速近似最近邻法FLANN, OpenCV包含完整的该算法，可实现k近邻的快速近似和匹配。

* 支持向量机SVM， 可分类也可回归。需要定义一个高维空间中任两点距离函数。(将数据投影到高维空
	间会使数据更容易线性可分) 该算法可以学习一个分类超平面，用来在高维空间里实现最优分类器。
	当数据有限的时候，该算法可以获得非常好的性能，而boosting和随机森林之恶能在拥有大量训练
	数据时才有好的效果。
	
* 人脸识别/级联分类器，物体检测算法，使用boosting算法。可以使用OpenCV提供的训练算法，也可
	以使用这个分类器提取自己想要的特征或使用其他的特征。该算法非常擅长检测特定视角的刚性物体。
	也被称为 "Viola-Jones分类器"

* Waldboost, Viola算法的衍生，是一个非常快的目标检测器，它在一系列任务中的表现都优于传统的
	级联分类器。

* 隐变量支持向量机(Latent SVM)， 是一个局部模型。通过识别物体的各个独立部分并学习一
	个模型去发现这些部分间的联系来识别复杂目标。

* 词袋模型， 在文档分类和视觉图像分类中有大量应用。不仅可以用来识别单个目标也可以识别场景和
	环境。

## 机器学习在视觉中的应用
以上算法都是使用特征构成的数据向量作为输入，特征数可达数千。
面临的问题: 
* 怎么采集数据并给数据定标签，
* 数据如何收集，(自己拍摄还是网上下载，是否需要采集运动信息，也要考虑场景中的其他因素的影响。
	总之需要捕获数据的各种变化: 不同视角，不同光线，不同天气，阴影等等)
* 怎么定标签, 如果有大量的数据，应该怎样给所有的图像定标签。例如在Amazon Mechanical Turk
	(MTurk)上找人打标签。或使用GPU或计算机集群通过计算机图像学计数渲染目标，人物，等。利用
	已经生成的数据和已知相机模型参数，得到非常逼真的图像。
* 需要提取哪些特征。对于人脸识别，如果人们总是正面出现，就没有必要使用旋转不变的特性，也没有
	必要预先旋转物体。总之，必须找到物体固有的属性特征，比如梯度直方图，色彩或SIFT特征。如
	果有背景信息，可以首先把背景去除，提取物体，然后进行图像处理(归一化图像，尺寸改变，旋转
	和直方图均衡)，计算很多特征。物体的特征向量将与物体的标签对应。
* 一旦数据被转换成特征向量，就可以吧数据分成训练集，验证集和测试集。
* 接下来就是选择分类器。一般需要考虑计算速度，数据形式和内存大小。建模时需要快速完成训练，此
	时**最邻近算法，朴素贝叶斯和决策树**都是不错的选择。如果需要考虑内存因素，**决策树和神
	经网络**是理想的选择。如果不需要训练很快，而是需要判断很快，但是要求精度很高，**boosting
	和随机森林**很符合。如果选取的特征比较好，仅需一个简单易懂的分类器，那么选择**决策树和最
	邻近算法**。但要想要好的性能，就离不开**boosting和随机森林**。

* 选择分类器时，要考虑我们的目的: 只是为了获得正确的分数，还是要对数据进行解释，是否寻求更小的
	内存或更快的速度，要求或决定的置信程度，不同的分类器面对这些需求有不同的表现。


## 变量的重要性算法
当我们拿到一个特征向量，我们怎么才能知道哪个特征对分类器的准确性有较大的贡献? 
* 二叉决策树，通过在每个节点选择最能分裂出数据的变量。最上面的变量是最重要的变量，往下依次类推。
* 随机森林
减少分类器需要考虑的特征的个数。可以提高速度。提高准确率。
Breiman的变量重要性算法:
1. 用训练集训练一个分类器
2. 使用验证集或测试集来确定分类器的准确率
3. 对每个数据样本，选择一个特征，随机选择其他特征数据中值来替补。保证特征的分布与原始
	数据集一致，但是特征的结构或意义被抹去。
4. 用改变后的训练集训练分类器，然后用改变后的测试集或验证集评价分类器。如果完全打乱特征会使
	正确率下降很多，说明这个特征很重要。反之可以不重要。
5. 重复以上3和4，检查所有特征。
这个过程内置在随机森林和决策树中。

## 诊断问题
一些规律: 
* 大量数据比少量数据好。
* 好的特征比好的算法重要。
* 选取的特征好，则独立性最大，在不同环境下的变化最小，那么大部分算法都可以有较好的效果。
一些问题:
* 1.欠拟合: 模型假设太严格使模型不能拟合到实际数据上
* 2.过拟合: 算法把噪声当作信号学习，导致泛化能力很差
* 3.错误: 学习一种模式时，机器学习的代码可以包含严重的错误，在预期完全错误时会降低计算性能。
* 4.训练和测试效果好，但实际应用效果差
可能的解决方案: 
* 1.使用更多的特征; 选用一个学习能力更强的算法
* 2.增加训练数据的数量; 减少特征的数量; 使用一个学习能力差一点的算法
* 4.采用更真实的数据
* 模型无法学习数据: 重新选择特征，使特征更能表达数据的不变特性; 使用更新，更相关的数据;换一
	个学习能力更强的拟合算法。

## 评估结果
在实际情况中，我们必须考虑噪声，采样误差和采样错误等因素。测试集或验证集可能并不能精确地反应
数据的真实分布。为了更准确地评估分类器性能，采用交叉验证(cross-validation)或bootstrapping。

交叉验证首先把数据分成k个不同的子集，然后用k-1个子集训练，另一个进行测试。k次(轮换)之后，把
结果平均。
bootstrapping与交叉验证相似，但是验证集是从训练集中随机选取的。选择的点仅用于测试，不用于训
练。做N次，每次随机选择验证集，最后算平均值。该效果一般胜于交叉验证。

### 评估和调整分类器
* ROC曲线: 显示分类器参数在所有的取值范围上变化是，正确识别率和错误识别率的关系。横轴是错误
	识别率，纵轴是正确识别率。该曲线越靠近左上角越好。可以计算ROC曲线下方的面积与真个ROC图
	的面积比，越接近1越好。
* 填充混淆矩阵: 显示错误识别率和错误拒绝率的关系(包含错误识别率，正确识别率，错误拒绝率，
	正确拒绝率)。理想情况下，左上和右下分别为100%, 其他为0。

### 分错类的代价
假设我们要设计一个分类器来检测毒蘑菇，我们希望 可食用的蘑菇被认为有毒(错误拒绝率) 很大??，
	有毒的蘑菇被认为可食用(错误识别率)很少。我们可以设置ROC参数使操作点下降，或生成ROC曲线
	时对错误识别赋予更大的权重。可以把每个错误识别的权值设为错误拒绝权值的10倍。一些OpenCV
	算法，比如决策树和SVM可以通过指定类别的先验概率或指定个别的训练样本的权重来自动平衡命中率
	和虚警率。

### 特征方差不一致
类似K近邻算法的一些算法会认为第一个特征相比于第二个特征可以忽略。解决方法是 预处理每个特征向
量，使其方差一致。如果特征不相关，这个步骤很重要。如果特征相关，可以用它们的协方差或平均方差
来归一化。决策树等算法不受方差是否一致的影响，可以不预先处理。如果算法以距离度量为准则，就需
要预先方差归一化。可以使用马氏距离来归一化特征方差。

### K均值
K-means 尝试找到数据的自然类别。(与EM算法和均值漂移算法 cv::MeanShift()相似。)在OpenCV中
采用的是Lloyd算法也叫Voronoi迭代。
1. 输入数据集和类别个数K
2. 随机分配类别中心点
3. 将每个点放入离他最近的中心点所在的集合
4. 移动中心点到集合中心
5. 重复3.4.知道收敛
K-means是一个及其高效的聚类算法。但它有如下问题:
* 它不能保证能找到定位聚类中心的最佳方案。但能保证收敛到某个解决方案。
* 它无法指出应该使用多少(类别)中心点。
* 它假设空间的协方差矩阵不会影响结果或已经归一化。
我们计算数据点到中心点的方差(正交和)，也叫紧凑度(compactness)。好的聚类能在不引起太大的复
杂度的情况下使方差最小。改进算法如下:
1. 对于同一个中心数K: 多进行几次K-means, 每次初始中心点不同，选择方差最小的那个。
2. 有多少类别数: 首先将类别数(中心数)设为1，然后提高类别数(当然有个上限)，每次聚类的时候
	选择方差最小的(上面方法1)那个。一般情况下，总方差会很快下降，直到到达一个拐点，这意味
	着一个新的聚类中心不会**显著**减少总方差。在拐点处停止，选择此时的类别数。
3. 将数据乘以逆协方差矩阵。例如: 如果输入向量是按行排列的，每行一个数据点，则计算新的数据
	向量D*, D* =D Sigma ^-(1/2)来归一化数据空间。
cv::kmeans()要求输入是一个多维数据点构成的矩阵(每行一个数据样本，数据的每个元素都是浮点型
CV_32FC1), 或简单的向量(每一列是一个多维点CV_32FC2,CV_32FC3,CV_32FC(M))。迭代停止可以是
达到最大迭代次数，也可以是中心点移动距离小于一个设定值。
初始分配中心点现在更常用cv::KMEANS_PP_CENTERS(flags), 该选项使用Sergei Vassilvitskii即
K-means++的方法，该方法更谨慎地选择起点，并通常比默认的随机中心迭代次数要少还能得到一个更
好的结果。

	
## 马氏距离
马氏距离在k-means中有两种用法。
* 可视为在拉伸的空间上测量的欧式距离。我们可以使用马氏距离对数据重新定标，可以显著提高K均
	值算法的性能。
* 还可以将新数据点分配个由K均值算法定义的聚类

### 使用马氏距离来约束输入数据
数据在空间中可能以不对成的方式分布。而K-means的核心观点是数据以不均匀方式聚类的，该算法试
图挖掘与这些聚类有关的信息。然而，**不对称**和**不均匀**之间存在重要的区别。
马氏距离可以使我们理解数据的协方差为数据空间的拉伸，通过对空间进行方差归一化，可以使数据在
不同的维度上均匀。例如不同的数据在不同维度上有不同的底层单位，身高，年龄，学龄都可以描述一个
人，但由于基本单位不同，数据在不同的维度上会有不同的分布。直接使用K-means的时候效果不好。

解决方法: 将数据当作一个整体，计算整个数据集的协方差矩阵。用它来重新调整整个数据集。
* 两个向量间的马氏距离等于 根号下 (向量差 的转置) 乘以 (协方差的逆(倒数)) 乘以向量差。
* 协方差等于 数据减去平均值 乘以 数据减去平均值 的转置 的期望 
	等于 (1/数据个数) 乘以 ((数据减去平均值) 乘以 ((数据减去平均值)的转置) 的和)
解决方式:
* 方式一: 在算法中使用马氏距离取代欧式距离
* 方式二: 将数据进行尺度变换，然后在缩放后的空间中就可以使用欧式距离。
方式二更容易计算，因为不用修改已经封装好的K-means算法。数据转换是线性的。

D* = D sigma^(-1/2)
对原始数据D进行逆协方差的平方根得到新的数据向量D*。ML库中数据D表示每个点的N行和M列，数据
是成行的。在这里，我们不适用cv::Mahalanobis()而是使用cv::calcCovarMatrix()计算集合的
协方差，使用cv::invert()(方法选用cv::DECOMP_EIG)来逆变换，最后计算平方根。这里计算平方
根可以通过先对角化然后取特征值的平方根，然后使用已经对角化的同一特征向量旋转回原始坐标系。
(对角化方法)

### 利用马氏距离分类
利用K-means聚类或其他方法给定一组聚类标签，利用这些标签预测新的数据点所有聚类。假设聚类
本身服从高斯分布。
首先根据平均协方差表示每个聚类。如此就可以计算任意一个新的数据点到每个聚类中心的马氏距离。
这里并不是有最小马氏距离就属于那个聚类，当所有聚类都有相同数量的数据点时，该结论才成立。
也就是说当聚类的点的数量不同时，每个聚类的权重也应该不同，该点出现在聚类旁边的概率就不同。
从贝叶斯规则的角度来看，马氏距离表示一个特定的样本来自一个特定的聚类的概率。我们想得到的是
当给定一个点x时，该点出现在聚类C中的概率。而马氏距离表示，给定聚类C，x出现在其中的概率。
根据贝叶斯公式，还需要聚类C的先验概率。
