# 机器学习基础

* 代码在opencv_contrib中
* 深度神经网络库cnn_3dobj和dnn
* OpenCV主要处理统计机器学习
* 机器学习中常见的任务**分类和聚类**与计算机视觉中最常见的两个任务**识别和分割**有共通之处。
* OpenCV的ML库: /opencv/modules/ml 和 /opencv/modules/flann

## 生成式模型和判别式模型
OpenCV倾向于支持判别式算法。该类算法是通过给定数据来判断类别的概率P(L|D)。而生成式算法
	是通过给定标签来生成数据的分布P(D|L)。判别式模型在根据给定数据做出预测方面更有优势，
	而生成式模型则具有更好的数据表达能力，在有条件的生成新数据方面也更有优势。

生成式模型通常根据给定的数据类别得到数据的条件分布，因此可以很容易看出这个分布跟实际的分布
	是否相似。生成式模型是对数据的产生进行建模，通常容易理解。而判别式模型则经常根据一些阙
	值作出决定。

## OpenCV的机器学习算法
* Mahalanobis算法， K-均值算法在core库中
* 目标识别和人脸识别在objdetect库中

* Mahalanobis, 一个距离度量。通过除以协方差来对数据空间进行变换，得到一个标志着空间拉伸程度
	的距离度量。如果协方差矩阵是单位矩阵则等价于欧式距离

* K-均值算法，非监督的聚类算法，用k个聚类中心表示数据的分布。该方法与期望最大化算法的区别在
	于k均值的中心不符合高斯分布。每个聚类中心都竞争者去俘获与之距离较近的点，所以聚类的结果
	就像肥皂泡，聚类区域经常被用作稀疏直方图的bin，用来描述数据。

* 正太/朴素贝叶斯，生成式的分类器。假设特征符合高斯分布并在统计学上服从独立同分布，但这个条
	件过于苛刻以至于在很多情况下不能被满足。因此被称为"朴素"贝叶斯分类器。尽管如此，它的效
	果仍然不错。

* 决策树，判别分类器。该树在当前节点通过寻找数据特征和一个阙值来实现数据到不同类别的最优划分。
	处理流程是不断将数据进行划分，然后下降到树的左侧子节点或右侧子节点。该算法性能不是最好的，
	但往往是测试的首选，它运行速度快，性能好。

* 期望最大化算法EM，一种用于聚类的无监督算法。它用N维高斯分布去拟合数据。仅用很少的几个参数
	(方差和均值) 就可以表示一个较为复杂的数据分布。该方法常用于分割。

* Boosting, 由一组判别分类器组成。最终的决策由组成它的各个分类器的加权组合来决定。在训练中，
	逐个训练子分类器。且每个子分类器都是弱分类器(性能上只是优于随机选择)。这些弱分类器由单
	变量决策树构成，被称为"桩"(stumps)。 在训练中，决策桩(decision stump)不仅从数据中学习
	分类决策而且根据预测精度学习"投票"的权重。当训练下一个分类器的时候，数据样本的权重会被重
	新分配。使之能够给与分错的数据更多的注意力。训练过程不停地执行，直到总错误(加权组合所有
	决策树组成的分类器产生的错误)低于预设的阙值。为了达到好的效果，这个方法通常需要很大量训
	练的训练数据。

* 随机森林，由很多决策树组成，每个决策树向下递归以获取最大深度。在学习过程中，每棵树的每个节
	点只从特征数据的一个随机子集中选择。这保证了每棵树是统计意义上不相关的分类器。在识别过程
	中，将每棵树的结果进行投票来确定最终结果，每棵树的权重相同。这个分类器方法经常很有效，而
	且对每棵树的输出进行平均，可以处理回归问题。

* k近邻，训练数据跟类别标签放在一起，离测试数据最近(欧氏距离)的k个样本进行投票，确定测试数据
	的分类结果。该方法通常比较有效，但是速度较慢且内存需求较大。

* 快速近似最近邻法FLANN, OpenCV包含完整的该算法，可实现k近邻的快速近似和匹配。

* 支持向量机SVM， 可分类也可回归。需要定义一个高维空间中任两点距离函数。(将数据投影到高维空
	间会使数据更容易线性可分) 该算法可以学习一个分类超平面，用来在高维空间里实现最优分类器。
	当数据有限的时候，该算法可以获得非常好的性能，而boosting和随机森林之恶能在拥有大量训练
	数据时才有好的效果。
	
* 人脸识别/级联分类器，物体检测算法，使用boosting算法。可以使用OpenCV提供的训练算法，也可
	以使用这个分类器提取自己想要的特征或使用其他的特征。该算法非常擅长检测特定视角的刚性物体。
	也被称为 "Viola-Jones分类器"

* Waldboost, Viola算法的衍生，是一个非常快的目标检测器，它在一系列任务中的表现都优于传统的
	级联分类器。

* 隐变量支持向量机(Latent SVM)， 是一个局部模型。通过识别物体的各个独立部分并学习一
	个模型去发现这些部分间的联系来识别复杂目标。

* 词袋模型， 在文档分类和视觉图像分类中有大量应用。不仅可以用来识别单个目标也可以识别场景和
	环境。

## 机器学习在视觉中的应用
以上算法都是使用特征构成的数据向量作为输入，特征数可达数千。
面临的问题: 
* 怎么采集数据并给数据定标签，
* 数据如何收集，(自己拍摄还是网上下载，是否需要采集运动信息，也要考虑场景中的其他因素的影响。
	总之需要捕获数据的各种变化: 不同视角，不同光线，不同天气，阴影等等)
* 怎么定标签, 如果有大量的数据，应该怎样给所有的图像定标签。例如在Amazon Mechanical Turk
	(MTurk)上找人打标签。或使用GPU或计算机集群通过计算机图像学计数渲染目标，人物，等。利用
	已经生成的数据和已知相机模型参数，得到非常逼真的图像。
* 需要提取哪些特征。对于人脸识别，如果人们总是正面出现，就没有必要使用旋转不变的特性，也没有
	必要预先旋转物体。总之，必须找到物体固有的属性特征，比如梯度直方图，色彩或SIFT特征。如
	果有背景信息，可以首先把背景去除，提取物体，然后进行图像处理(归一化图像，尺寸改变，旋转
	和直方图均衡)，计算很多特征。物体的特征向量将与物体的标签对应。
* 一旦数据被转换成特征向量，就可以吧数据分成训练集，验证集和测试集。
* 接下来就是选择分类器。一般需要考虑计算速度，数据形式和内存大小。建模时需要快速完成训练，此
	时**最邻近算法，朴素贝叶斯和决策树**都是不错的选择。如果需要考虑内存因素，**决策树和神
	经网络**是理想的选择。如果不需要训练很快，而是需要判断很快，但是要求精度很高，**boosting
	和随机森林**很符合。如果选取的特征比较好，仅需一个简单易懂的分类器，那么选择**决策树和最
	邻近算法**。但要想要好的性能，就离不开**boosting和随机森林**。

* 选择分类器时，要考虑我们的目的: 只是为了获得正确的分数，还是要对数据进行解释，是否寻求更小的
	内存或更快的速度，要求或决定的置信程度，不同的分类器面对这些需求有不同的表现。


## 变量的重要性算法
当我们拿到一个特征向量，我们怎么才能知道哪个特征对分类器的准确性有较大的贡献? 
* 二叉决策树，通过在每个节点选择最能分裂出数据的变量。最上面的变量是最重要的变量，往下依次类推。
* 随机森林
减少分类器需要考虑的特征的个数。可以提高速度。提高准确率。
Breiman的变量重要性算法:
1. 用训练集训练一个分类器
2. 使用验证集或测试集来确定分类器的准确率
3. 对每个数据样本，选择一个特征，随机选择其他特征数据中值来替补。保证特征的分布与原始
	数据集一致，但是特征的结构或意义被抹去。
4. 用改变后的训练集训练分类器，然后用改变后的测试集或验证集评价分类器。如果完全打乱特征会使
	正确率下降很多，说明这个特征很重要。反之可以不重要。
5. 重复以上3和4，检查所有特征。
这个过程内置在随机森林和决策树中。

## 诊断问题
一些规律: 
* 大量数据比少量数据好。
* 好的特征比好的算法重要。
* 选取的特征好，则独立性最大，在不同环境下的变化最小，那么大部分算法都可以有较好的效果。
一些问题:
* 1.欠拟合: 模型假设太严格使模型不能拟合到实际数据上
* 2.过拟合: 算法把噪声当作信号学习，导致泛化能力很差
* 3.错误: 学习一种模式时，机器学习的代码可以包含严重的错误，在预期完全错误时会降低计算性能。
* 4.训练和测试效果好，但实际应用效果差
可能的解决方案: 
* 1.使用更多的特征; 选用一个学习能力更强的算法
* 2.增加训练数据的数量; 减少特征的数量; 使用一个学习能力差一点的算法
* 4.采用更真实的数据
* 模型无法学习数据: 重新选择特征，使特征更能表达数据的不变特性; 使用更新，更相关的数据;换一
	个学习能力更强的拟合算法。

## 评估结果
在实际情况中，我们必须考虑噪声，采样误差和采样错误等因素。测试集或验证集可能并不能精确地反应
数据的真实分布。为了更准确地评估分类器性能，采用交叉验证(cross-validation)或bootstrapping。

交叉验证首先把数据分成k个不同的子集，然后用k-1个子集训练，另一个进行测试。k次(轮换)之后，把
结果平均。
bootstrapping与交叉验证相似，但是验证集是从训练集中随机选取的。选择的点仅用于测试，不用于训
练。做N次，每次随机选择验证集，最后算平均值。该效果一般胜于交叉验证。

### 评估和调整分类器
* ROC曲线: 显示分类器参数在所有的取值范围上变化是，正确识别率和错误识别率的关系。横轴是错误
	识别率，纵轴是正确识别率。该曲线越靠近左上角越好。可以计算ROC曲线下方的面积与真个ROC图
	的面积比，越接近1越好。
* 填充混淆矩阵: 显示错误识别率和错误拒绝率的关系(包含错误识别率，正确识别率，错误拒绝率，
	正确拒绝率)。理想情况下，左上和右下分别为100%, 其他为0。

### 分错类的代价
假设我们要设计一个分类器来检测毒蘑菇，我们希望 可食用的蘑菇被认为有毒(错误拒绝率) 很大??，
	有毒的蘑菇被认为可食用(错误识别率)很少。我们可以设置ROC参数使操作点下降，或生成ROC曲线
	时对错误识别赋予更大的权重。可以把每个错误识别的权值设为错误拒绝权值的10倍。一些OpenCV
	算法，比如决策树和SVM可以通过指定类别的先验概率或指定个别的训练样本的权重来自动平衡命中率
	和虚警率。

### 特征方差不一致
类似K近邻算法的一些算法会认为第一个特征相比于第二个特征可以忽略。解决方法是 预处理每个特征向
量，使其方差一致。如果特征不相关，这个步骤很重要。如果特征相关，可以用它们的协方差或平均方差
来归一化。决策树等算法不受方差是否一致的影响，可以不预先处理。如果算法以距离度量为准则，就需
要预先方差归一化。可以使用马氏距离来归一化特征方差。



	
